#!/bin/bash

# This is a script to run the moongen benchmark
# Author: Andrew Theurer

# This script attempts to automate potentially a very large number of tests for moongen

# This script will take multiple samples of the same test type and try to achieve a standard deviation of <3%
#
# This script will repeat a test type 6 times in order to try to achieve target stddev.
# If a run (with several samples) fails the stddev, its directory is appended with -fail
#
# This script will also generate a "summary-results.txt" with a table of all
# results, efficiency, and other stats.

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark_name="moongen"
benchmark_bin=/usr/local/bin/$benchmark_name

# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

# Defaults

benchmark_run_dir=""
#test_types="unidirec,bidirec"
test_types="unidirec"
max_drop_pcts="0"
frame_sizes="64,256,512,1024,1480" # in bytes
frame_sizes="64" # in bytes
config=""
nr_flows="1024"
nr_samples=5
maxstddevpct=5 # maximum allowable standard deviation in percent
max_failures=6 # after N failed attempts to hit below $maxstddevpct, move on to the nest test
runtime=60
servers=127.0.0.1
postprocess_only=n
log_response_times=n
start_iteration_num=1
keep_failed_tool_data="y"
tar_nonref_data="n"
orig_cmd="$*"
tool_group=default
tool_label_pattern="$benchmark_name-"
install_only="n"

# Process options and arguments
opts=$(getopt -q -o i:c:t:r:m:p:M:S:C: --longoptions "max-drop-pct:,max-drop-pcts:,client-label:,server-label:,tool-label-pattern:,install,start-iteration-num:,config:,nr-flows:,test-types:,runtime:,frame-sizes:,samples:,client:,clients:,servers:,server:,max-stddev:,max-failures:,log-response-times:,postprocess-only:,run-dir:,tool-group:" -n "getopt.sh" -- "$@")
if [ $? -ne 0 ]; then
	printf -- "$*\n"
	printf "\n"
	printf "\t${benchmark_name}: you specified an invalid option\n\n"
	printf "\tThe following options are available:\n\n"
	#              1   2         3         4         5         6         7         8         9         0         1         2         3
	#              678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012
	printf -- "\t\t             --kvm-host=str\n"
	printf -- "\t\t             --tool-group=str\n"
	printf -- "\t\t-c str       --config=str               name of the test config (i.e. jumbo_frames_and_network_throughput)\n"
	printf -- "\t\t-t str[,str] --test-types=str[,str]     list of one or more: unidirec or bidirec (default $test_types)\n"
	printf -- "\t\t-r int       --runtime=int              test measurement period in seconds (default is $runtime)\n"
	printf -- "\t\t             --max-drop_pct[s]=fl,[fl]  list of maximum allowed percentage of dropped frames (default is $max_drop_pcts)\n"
	printf -- "\t\t-m int[,int] --frame-sizes=str[,str]    list of frame sizes in bytes (default is $frame_sizes)\n"
	printf -- "\t\t-i int[,int] --nr-flows=int[,int]       list of number of packet flows to run (default is $nr_flows)\n"
	printf -- "\t\t             --samples=int              the number of times each different test is run (to compute average &\n"
	printf    "\t\t                                        standard deviations)\n"
	printf -- "\t\t             --max-failures=int         the maximm number of failures to get below stddev\n"
	printf -- "\t\t             --max-stddev=int           the maximm percent stddev allowed to pass\n"
	printf -- "\t\t             --postprocess-only=y|n     don't run the benchmark, but postprocess data from previous test\n"
	printf -- "\t\t             --run-dir=str              optionally specify what directory should be used (usually only used\n"
	printf    "\t\t                                        if postprocess-only=y)\n"
	printf -- "\t\t             --start-iteration-num=int  optionally skip the first (n-1) tests\n"
	printf -- "\t\t             --log-response-times=y|n   record the response time of every single operation\n"
	printf -- "\t\t             --tool-label-pattern=str   $benchmark_name will provide CPU and efficiency information for any tool directory\n"
	printf    "\t\t                                        with a \"^<pattern>\" in the name, provided \"sar\" is one of the\n"
	printf    "\t\t                                        registered tools.\n"
	printf    "\t\t                                        a default pattern, \"$bechmark_name-\" is used if none is provided.\n"
	printf    "\t\t                                        simply register your tools with \"--label=$benchmark_name-\$X\", and this script\n"
	printf    "\t\t                                        will genrate CPU_$benchmark_name-\$X and Gbps/CPU_$benchmark_name-\$X or\n"
	printf    "\t\t                                        trans_sec/CPU-$benchmark_name-\$X for all tools which have that pattern as a\n"
	printf    "\t\t                                        prefix.  if you don't want to register your tools with \"$benchmark_name-\" as\n"
	printf    "\t\t                                        part of the label, just use --tool-label-pattern= to tell this script\n"
	printf    "\t\t                                        the prefix pattern to use for CPU information.\n"
	exit 1
fi
eval set -- "$opts"
debug_log "[$script_name]processing options"
while true; do
	case "$1" in
		--install)
		shift
		install_only="y"
		exit
		;;
		--tool-label-pattern)
		shift
		if [ -n "$1" ]; then
			tool_label_pattern="$1"
			shift
		fi
		;;
		--postprocess-only)
		shift
		if [ -n "$1" ]; then
			postprocess_only="$1"
			shift
		fi
		;;
		--run-dir)
		shift
		if [ -n "$1" ]; then
			benchmark_run_dir="$1"
			shift
		fi
		;;
		--max-stddev)
		shift
		if [ -n "$1" ]; then
			maxstddevpct="$1"
			shift
		fi
		;;
		--max-failures)
		shift
		if [ -n "$1" ]; then
			max_failures="$1"
			shift
		fi
		;;
		--samples)
		shift
		if [ -n "$1" ]; then
			nr_samples="$1"
			shift
		fi
		;;
		-i|--nr-flows)
		shift
		if [ -n "$1" ]; then
			nr_flows="$1"
			shift
		fi
		;;
		--max-drop-pct|--max-drop-pcts)
		shift
		if [ -n "$1" ]; then
			max_drop_pcts="$1"
			shift
		fi
		;;
		-t|--test-types)
		shift
		if [ -n "$1" ]; then
			test_types="$1"
			shift
		fi
		;;
		--tool-group)
		shift
		if [ -n "$1" ]; then
			tool_group="$1"
			shift
		fi
		;;
		-m|--frame-sizes)
		shift
		if [ -n "$1" ]; then
			frame_sizes="$1"
			shift
		fi
		;;
		-r|--runtime)
		shift
		if [ -n "$1" ]; then
			runtime="$1"
			shift
		fi
		;;
		--log-response-times)
		shift
		if [ -n "$1" ]; then
			log_response_times="$1"
			shift
		fi
		;;
		-c|--config)
		shift
		if [ -n "$1" ]; then
			config="$1"
			shift
		fi
		;;
		--start-iteration-num)
		shift
		if [ -n "$1" ]; then
			start_iteration_num=$1
			shift
		fi
		;;
		--)
		shift
		break
		;;
		*)
		error_log "[$script_name] bad option, \"$1 $2\""
		break
		;;
	esac
done
if [[ -z "$benchmark_run_dir" ]]; then
	# We don't have an explicit run directory, construct one
	benchmark_run_dir="$pbench_run/${benchmark_name}_${config}_$date"
else
	# We have an explicit run directory provided by --run-dir, so warn
	# the user if they also used --config
	if [[ ! -z "$config" ]]; then
		warn_log "[$script_name] ignoring --config=\"$config\" in favor of --rundir=\"$benchmark_run_dir\""
	fi
fi
mkdir -p $benchmark_run_dir/.running

verify_tool_group $tool_group
register-tool-trigger --group=$tool_group --start-trigger="Starting final validation" --stop-trigger="Stopping final validation" 

count=0
for test_type in `echo $test_types | sed -e s/,/" "/g`; do
	for frame_size in `echo $frame_sizes | sed -e s/,/" "/g`; do
		for nr_flow in `echo $nr_flows | sed -e s/,/" "/g`; do
			let count=$count+1
		done
	done
done

let total_iterations=count
count=1
mkdir -p $benchmark_run_dir/.running
export benchmark_name config
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg
# start the server processes
for test_type in `echo $test_types | sed -e s/,/" "/g`; do
	for max_drop_pct in `echo $max_drop_pcts | sed -e s/,/" "/g`; do
		for frame_size in `echo $frame_sizes | sed -e s/,/" "/g`; do
			for nr_flow in `echo $nr_flows | sed -e s/,/" "/g`; do
				if [ $count -ge $start_iteration_num ]; then
					iteration="${count}-${test_type}-${frame_size}B-${nr_flow}flows-${max_drop_pct}pct_drop"
					iteration_dir="$benchmark_run_dir/$iteration"
					result_stddevpct=$maxstddevpct # this test case will get a "do-over" if the stddev is not low enough
					failures=0
					echo "Starting iteration[$iteration] ($count of $total_iterations)"
					log "Starting iteration[$iteration] ($count of $total_iterations)"
					while [[ $(echo "if (${result_stddevpct} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; do
						if [[ $failures -gt 0 ]]; then
							echo "Restarting iteration[$iteration] ($count of $total_iterations)"
							log "Restarting iteration[$iteration] ($count of $total_iterations)"
						fi
						mkdir -p $iteration_dir
						# each attempt at a test config requires multiple samples to get stddev
						for sample in `seq 1 $nr_samples`; do
							benchmark_results_dir="$iteration_dir/sample$sample"
							if [ "$postprocess_only" != "y" ]; then
								mkdir -p $benchmark_results_dir
								echo "test sample $sample of $nr_samples"
								log "test sample $sample of $nr_samples "
								benchmark_client_cmd_file="$iteration_dir/$benchmark_name-client.cmd"
								result_file=$benchmark_results_dir/moongen-result.txt
								debug_log "client[$client]${client_nodeinfo}test[$test_type]flows[$nr_flow]size[$frame_size] <-> server[$server]${server_nodeinfo}"
								echo "client[$client]${client_nodeinfo}test[$test_type]flows[$nr_flow]size[$frame_size] <-> server[$server]${server_nodeinfo}"
								pushd /root/MoonGen
								if [ $test_type == "unidirec" ]; then
									bidirec_opt=0
								else
									bidirec_opt=1
								fi
								# save benchmark command in file for debugging or running manually
								echo "pushd /root/MoonGen" >$benchmark_client_cmd_file
								echo "./build/MoonGen ./examples/opnfv-vsperf.lua 0 1 $frame_size $bidirec_opt $max_drop_pct $nr_flow" >>$benchmark_client_cmd_file
								chmod +x $benchmark_client_cmd_file
								# there is no "start-tools" before starting the benchmark because this benchmark uses tool-trigger to start the tools
								$benchmark_client_cmd_file | tool-trigger $iteration $benchmark_results_dir | tee $result_file
								echo exit code: $?
								# there is no "stop-tools" before starting the benchmark because this benchmark uses tool-trigger to stop the tools
								postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
							else
								if [[ ! -d $benchmark_results_dir ]]; then
									error_log "Results directory $benchmark_results_dir does not exist, skipping post-processing"
									continue
								fi
								echo "Not going to run $benchmark_name.  Only postprocesing existing data"
								log "Not going to run $benchmnark_name.  Only postprocesing existing data"
							fi
							$script_path/postprocess/$benchmark_name-postprocess $benchmark_results_dir $iteration "$tool_label_pattern" "$tool_group"
						done
						# process_benchmark_iteration_samples "$iteration_dir" "$nr_samples" "$maxstddevpct" "$primary_metric" "$failures" "$max_failures"
						# fail=$?
						fail=0
						if [ $fail -eq 1 ]; then
							((failures++))
						fi
						if [ $fail -eq 0 -o $failures -ge $max_failures ]; then
							break
						fi
					done # break out of this loop only if the $result_stddevpct is lower than $maxstddevpct
					echo "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
					log "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				else
					echo "Skipping iteration $iteration ($count of $total_iterations)"
					log "Skipping iteration $iteration ($count of $total_iterations)"
				fi
				last_test_type="$test_type"
				let count=$count+1 # now we can move to the next iteration
			done
		done
	done
done
$script_path/postprocess/generate-benchmark-summary "$benchmark_name" "$orig_cmd" "$benchmark_run_dir"
collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end
rmdir $benchmark_run_dir/.running
