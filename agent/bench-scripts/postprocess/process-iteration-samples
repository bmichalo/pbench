#!/usr/bin/perl
# Author: Andrew Theurer
# Post-process all of the sample results for any benchmark

use strict;
use warnings;

# Check for an alternate tools library path for testing
my $_test_alt_tools_lib;
my $_test_alt_bench_lib;
BEGIN {
	my $_pbench_tspp_dir = $ENV{'pbench_tspp_dir'};
	$_test_alt_tools_lib=$ENV{_TEST_ALTERNATE_TOOLS_LIBRARY};
	if (not defined $_test_alt_tools_lib or not -d $_test_alt_tools_lib) {
		$_test_alt_tools_lib = "$_pbench_tspp_dir";
	}
	my $_pbench_bspp_dir = $ENV{'pbench_bspp_dir'};
	$_test_alt_bench_lib=$ENV{_TEST_ALTERNATE_BENCH_LIBRARY};
	if (not defined $_test_alt_bench_lib or not -d $_test_alt_bench_lib) {
		$_test_alt_bench_lib = "$_pbench_bspp_dir";
	}
}
use lib "$_test_alt_tools_lib";
use lib "$_test_alt_bench_lib";
no lib ".";
use BenchPostprocess qw(get_cpubusy_series calc_ratio_series calc_sum_series);
use File::Basename;
use Data::Dumper;
use List::Util('sum');
use JSON;

# This script produces a JSON array containing the information for
# 1 benchmark iteration, wich can include 1 or more run samples.
# This uses the standard metric types for pbench, which are:
# %workload{parameters,throughput|latency|resource|efficiency}
#
# However, metric types throughput, latency, resource, and efficiency
# contain a samples[] array, which includes those metrics from those
# test samples.  
my %workload;   # root hash for all data, contains hash refs to
		# %paramters, %throughput, %latency, %resource, %efficiency
		
my %parameters;	# a hash of parameter-type:parameter-value that was
		# $parameters{benchmark[0].rate}
my @benchmark;	# each array element contains a hash with:
		# benchmark_name:
	    	# :
	    	# role:
	    	# description:

my %resource;	# a hash of resource-type:array-of-resource-values,
		# for example $resource{cpu_busy[0..1]}
#my @cpu_busy;	# each array element contains a hash with
		# hostname: hostname or IP
		# role: client, server, host, kvm-host, container-host, etc.
		# timeseries: a hash of timestamp:value key-pairs

my %efficiency; # a hash of throughput-per-resource-type:array-of-throughput-per-resource-values
		# for example $efficincy{Mframes_cpu[0..1]}
#my @Mframes_cpu;# each array element contains a hash with:
		# hostname:
	    	# port_id:
	    	# role:
	    	# description:
	    	# timeseries: a hash of timestamp,value elements

my %latency;    # a hash of latency-type:array-of-latency-values,
		# for example $latency{usec[0..1]}
#my @usec;	# each array element contains a hash with:
	    	# hostname:
	    	# port_id:
	    	# role:
	    	# description:
	    	# timeseries: a hash of timestamp,value elements

my %throughput; # a hash of throughput-type:array-of-throughput-values,
		# for example $throughput{Mframes_sec[0..1]
		# The throughput-type can vary from one benchmarkto another
		# For example, dbench may use "MB_sec" and uperf may use "Gb_sec"
		#
		# each array element contains a hash with:
	    	# hostname:
	    	# port_id:
	    	# role: packet-generator
	    	# mean: the mean (average) of the samples' results
	    	# stddevpct: a percent standard deviation for the result based on the samples' results

my $script = basename($0);
my $dir = $ARGV[0];  # The iteration directory
my $primary_metric = $ARGV[1]; # this metric is used to test against maximum allowed stddev
my $max_stddevpct = $ARGV[2]; 
my $num_failures = $ARGV[3]; # number of times a set of samples have failed the max allowed stddev
my $max_failures = $ARGV[4]; # the maxium number of failures allowed
my $tar_nonref_data = $ARGV[5]; # (y/n) tar/compress the data samples that are not the reference result?
my $keep_failed_tool_data = $ARGV[6]; # (y/n) keep the tool data for a set of samples that failed?
my @json_samples;

sub get_avg_stddev {
        my @numbers = @_;
	my $count = scalar @numbers;
	my $sum = 0;

	if ($count == 1) {
		return ($numbers[0]{'value'}, 0, 0, 1);
	} elsif ($count == 0) {
		return (0, 0, 0, 0);
	} else {
		my $val;
		my $sum;
		my $index;
		my $sqdiff;
		my $closest_index;
		my $mindiff;
		my $totalsqdiff;
		my $stddev;
		my $stddevpct;
		my $avg;
		my $i;
		for ($i=0; $i < scalar @numbers; $i++) {
			$val = $numbers[$i]{'value'};
			$sum += $val;
		}
		$avg = $sum / $count;
		
		$index = 1;
		for ($i=0; $i < scalar @numbers; $i++) {
			$val = $numbers[$i]{'value'};
			$sqdiff = ($avg - $val)**2;
		
			# keep track of the value which has the least difference from the average
			if (!defined $closest_index) {
				$mindiff = $sqdiff;
				$closest_index = $index;
			}
			elsif ( $sqdiff < $mindiff ) {
				$mindiff = $sqdiff;
				$closest_index = $index;
			}
			$totalsqdiff += $sqdiff;
			$index++;
		}
		$stddev = ($totalsqdiff / ($count)) ** 0.5;
		# if $stddev is 0 and the avgerage is 0, then return 0 for $stddevpct (do not divide by zero below)
		if (($stddev == 0) && ($avg == 0)) {
				$stddevpct = 0;
		} else {
			$stddevpct = 100 * $stddev / $avg;
		}
		return (0 + $avg, 0 + $stddev, 0 + $stddevpct, 0 + $closest_index);
	}
}
	
opendir(my $dh, $dir) || die "$script: could not open directory $dir: $!\n";
my $file;
foreach $file ( readdir($dh) ) {
	if ($file =~ /sample(\d+)?/) {
		my $sample_id = $1;
		my $sample_file = "$dir/sample$sample_id/result.json";
		open( TXT, "<$sample_file" ) or die "Can't open $sample_file: $!";
		my $json_text;
		while ( <TXT> ) {
			$json_text = $json_text . $_;
			}
		close TXT;
		my $perl_scalar = from_json( $json_text, { utf8  => 1 } );
		push @json_samples, $perl_scalar;
		}
}

# build our iteration structure which will be converted to JSON
my %iteration;
my %processing;
my $sample;
foreach $sample (@json_samples) { # samples 0..N
	my %sample_hash = %{$sample};
	my $sample_hash_key;
	foreach $sample_hash_key (keys %sample_hash) {
		# within each sample, there should be at least a "parameters" and "throughput" key, and possiblly
		# three other keys: resource, latency, and efficiency
		# my @metric_types = ("parameters", "throughput", "resource", "latency", "efficiency");
		# For each type of parameter, there is an array of hashes.  For example:
		# $sample_hash{"parameters"}{"benchmark"}[0]  .This has key:value pairs for things
		# like message_size_bytes:64 or num_flows:1024. There may be only one hash in this
		# array, but it's possible to have more (for a potential multi-benchmark-type workload)
		my $parameter_type;
		foreach $parameter_type (keys $sample_hash{$sample_hash_key}) {
			#if (! $iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{'samples'}) {
			#$iteration{$sample_hash_key}{$parameter_type} = \@{ $sample_hash{$sample_hash_key}{$parameter_type} };
		#}
			my $parameter_type_idx;
			for ($parameter_type_idx = 0; $parameter_type_idx < scalar @{$sample_hash{$sample_hash_key}{$parameter_type}}; $parameter_type_idx++) {
				my $key;
				foreach $key (keys %{$sample_hash{$sample_hash_key}{$parameter_type}[$parameter_type_idx]}) {
					my $sample_value = $sample_hash{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{$key};
					# There are a few keys which are not simply duplicates across all samples.
					# For example, $json_samples[1]{'throughput'}{Gb_sec}[0].mean is the average Gigabits per second for that test sample.
					# Other test samples for this benchmark iteration also contain a value like that, but the actual value differs (usually slightly if it is a consistent workload).
					# So, for the iteration hash, we store all of the "value" values from all samples, and compute a mean of
					# those samples, as well as a standard devivation percent.
					if ($key eq "value") {
						my %samples_hash;
						$samples_hash{'value'} = $sample_value;
						if ($sample_hash{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{'timeseries'}) {
							$samples_hash{'timeseries'} = $sample_hash{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{'timeseries'};
						}
						if (! $iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{'samples'}) {
							# Create an array to hold the samples for "value" values
							# For example, if uperf had 5 sample runs, there would be an array of 5 Gb_sec values in this array
							#my @samples;
							$iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{'samples'} = ();
						}
						push @{$iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{'samples'}}, \%samples_hash;
					# For the rest of the keys, either chack to see if it matches existing value in the %iteration, 
					# or if it does not exist, create it.
					} else {
						if ($key ne "timeseries") {
							if ($iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{$key}) {
								my $iteration_value = $iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{$key};
								if ($sample_value ne $iteration_value) {
									print "Error: values do not match\nkey: $key values: $iteration_value $sample_value\n";
								}
							} else {
								#print "assigning new key/value\n";
								$iteration{$sample_hash_key}{$parameter_type}[$parameter_type_idx]{$key} = $sample_value;
							}
						}
					}
				}
			}
		}
	}
}
my $fail = 0;
my $hash_key;
foreach $hash_key (keys %iteration) {
	my $parameter_type;
	foreach $parameter_type (keys $iteration{$hash_key}) {
		my $parameter_type_idx;
		for ($parameter_type_idx = 0; $parameter_type_idx < scalar @{$iteration{$hash_key}{$parameter_type}}; $parameter_type_idx++) {
			my $key;
			foreach $key (keys %{$iteration{$hash_key}{$parameter_type}[$parameter_type_idx]}) {
				if ($key eq "samples") {
					# compute mean, stabdard deviation, and the sample value that is closest to the mean value
					($iteration{$hash_key}{$parameter_type}[$parameter_type_idx]{'mean'},
					 $iteration{$hash_key}{$parameter_type}[$parameter_type_idx]{'stddev'},
					 $iteration{$hash_key}{$parameter_type}[$parameter_type_idx]{'stddevpct'},
					 $iteration{$hash_key}{$parameter_type}[$parameter_type_idx]{'closest-sample'}) = get_avg_stddev(@{$iteration{$hash_key}{$parameter_type}[$parameter_type_idx]{'samples'}});
					if ($hash_key eq "throughput" && $parameter_type eq $primary_metric && $iteration{$hash_key}{$parameter_type}[$parameter_type_idx]{'stddevpct'} > $max_stddevpct) {
						$fail = 1;
					}
				}
			}
		}
	}
}
my $sample_num;
for ($sample_num = 1; $sample_num < scalar @{$iteration{'throughput'}{$primary_metric}[0]{'samples'}} + 1; $sample_num++) {
	if ($sample_num == $iteration{'throughput'}{$primary_metric}[0]{'closest-sample'}) {
		# use "reference-result" to point to result which is closest to the average
		system("ln -sf sample$sample_num reference-result");
	} else {
		if ($tar_nonref_data eq "y") {
			system("tar --directory=$dir --create --xz --force-local --file=sample$sample_num.tar.xz sample$sample_num; /bin/rm -rf sample$sample_num");
		}
	}
	if ($fail && $keep_failed_tool_data eq "n") {
		system("/bin/rm -rf \"$dir/sample$sample_num/tools-default\"");
	}
}

my $json_file = $dir . "/result.json";
my $json_text   = to_json( \%iteration, { ascii => 1, pretty => 1 } );
open(JSON, ">$json_file") || die "$script: could not open file $json_file: $!\n";
print JSON $json_text;
close(JSON);
	
if ($fail) {
	print "This iteration's standard deviation percentage was not within $max_stddevpct\n";
	$num_failures++;
	rename $dir, $dir."-fail".$num_failures;
}
if ($fail && $num_failures < $max_failures) {
	# signal that the iteration should be retried
	exit 1
} else {
	exit 0
}
