#!/bin/bash

# This is a script to run the moongen benchmark
# Author: Andrew Theurer

# This script attempts to automate potentially a very large number of tests for moongen

# This script will take multiple samples of the same test type and try to achieve a standard deviation of <3%
#
# This script will repeat a test type 6 times in order to try to achieve target stddev.
# If a run (with several samples) fails the stddev, its directory is appended with -fail
#
# This script will also generate a "summary-results.txt" with a table of all
# results, efficiency, and other stats.

script_path=`dirname $0`
script_name=`basename $0`
pbench_bin="`cd ${script_path}/..; /bin/pwd`"

# source the base script
. "$pbench_bin"/base

benchmark_name="moongen"
moongen_dir="/root/MoonGen"
benchmark_bin=/usr/local/bin/$benchmark_name

# Every bench-script follows a similar sequence:
# 1) process bench script arguments
# 2) ensure the right version of the benchmark is installed
# 3) gather pre-run state
# 4) run the benchmark and start/stop perf analysis tools
# 5) gather post-run state
# 6) postprocess benchmark data
# 7) postprocess analysis tool data

# Defaults
cfg_file="opnfv-vsperf-cfg.lua"
benchmark_run_dir=""
test_types="unidirec"
max_drop_pcts="0"
start_rate_mpps=""
frame_sizes="64,256,512,1024,1480" # in bytes
frame_sizes="64" # in bytes
config=""
nr_flows="1024"
ur_samples=5
maxstddevpct=5 # maximum allowable standard deviation in percent
max_failures=6 # after N failed attempts to hit below $maxstddevpct, move on to the nest test
search_runtime=60
validation_runtime=120
servers=127.0.0.1
postprocess_only=n
log_response_times=n
start_iteration_num=1
keep_failed_tool_data="y"
tar_nonref_data="n"
orig_cmd="$*"
tool_group=default
tool_label_pattern="$benchmark_name-"
install_only="n"

# Process options and arguments
opts=$(getopt -q -o i:c:t:r:m:p:M:S:C: --longoptions "portlist:,start-rate-mpps:,max-drop-pct:,max-drop-pcts:,client-label:,server-label:,tool-label-pattern:,install,start-iteration-num:,config:,nr-flows:,test-types:,search-runtime:,validation-runtime:,frame-sizes:,samples:,client:,clients:,servers:,server:,max-stddev:,max-failures:,log-response-times:,postprocess-only:,run-dir:,tool-group:" -n "getopt.sh" -- "$@")
if [ $? -ne 0 ]; then
	printf -- "$*\n"
	printf "\n"
	printf "\t${benchmark_name}: you specified an invalid option\n\n"
	printf "\tThe following options are available:\n\n"
	#              1   2         3         4         5         6         7         8         9         0         1         2         3
	#              678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012
	printf -- "\t\t             --kvm-host=str\n"
	printf -- "\t\t             --tool-group=str\n"
	printf -- "\t\t-c str       --config=str               name of the test config (i.e. jumbo_frames_and_network_throughput)\n"
	printf -- "\t\t             --portlist=int,int[,int,int]  list of DPDK ports to use (multiples of 2)\n"
	printf -- "\t\t-t str[,str] --test-types=str[,str]     list of one or more: unidirec or bidirec (default $test_types)\n"
	printf -- "\t\t-r int       --search-runtime=int       test measurement period in seconds when searching for max throughput (default is $search-runtime)\n"
	printf -- "\t\t-r int       --validation-runtime=int   test measurement period in seconds when running final validation (default is $validation-runtime)\n"
	printf -- "\t\t             --max-drop-pct[s]=fl,[fl]  list of maximum allowed percentage of dropped frames (default is $max_drop_pcts)\n"
	printf -- "\t\t-m int[,int] --frame-sizes=str[,str]    list of frame sizes in bytes (default is $frame_sizes)\n"
	printf -- "\t\t-i int[,int] --nr-flows=int[,int]       list of number of packet flows to run (default is $nr_flows)\n"
	printf -- "\t\t             --samples=int              the number of times each different test is run (to compute average &\n"
	printf    "\t\t                                        standard deviations)\n"
	printf -- "\t\t             --max-failures=int         the maximm number of failures to get below stddev\n"
	printf -- "\t\t             --max-stddev=int           the maximm percent stddev allowed to pass\n"
	printf -- "\t\t             --postprocess-only=y|n     don't run the benchmark, but postprocess data from previous test\n"
	printf -- "\t\t             --run-dir=str              optionally specify what directory should be used (usually only used\n"
	printf    "\t\t                                        if postprocess-only=y)\n"
	printf -- "\t\t             --start-iteration-num=int  optionally skip the first (n-1) tests\n"
	printf -- "\t\t             --log-response-times=y|n   record the response time of every single operation\n"
	printf -- "\t\t             --tool-label-pattern=str   $benchmark_name will provide CPU and efficiency information for any tool directory\n"
	printf    "\t\t                                        with a \"^<pattern>\" in the name, provided \"sar\" is one of the\n"
	printf    "\t\t                                        registered tools.\n"
	printf    "\t\t                                        a default pattern, \"$bechmark_name-\" is used if none is provided.\n"
	printf    "\t\t                                        simply register your tools with \"--label=$benchmark_name-\$X\", and this script\n"
	printf    "\t\t                                        will genrate CPU_$benchmark_name-\$X and Gbps/CPU_$benchmark_name-\$X or\n"
	printf    "\t\t                                        trans_sec/CPU-$benchmark_name-\$X for all tools which have that pattern as a\n"
	printf    "\t\t                                        prefix.  if you don't want to register your tools with \"$benchmark_name-\" as\n"
	printf    "\t\t                                        part of the label, just use --tool-label-pattern= to tell this script\n"
	printf    "\t\t                                        the prefix pattern to use for CPU information.\n"
	exit 1
fi
eval set -- "$opts"
debug_log "[$script_name]processing options"
while true; do
	case "$1" in
		--install)
		shift
		install_only="y"
		exit
		;;
		--tool-label-pattern)
		shift
		if [ -n "$1" ]; then
			tool_label_pattern="$1"
			shift
		fi
		;;
		--postprocess-only)
		shift
		if [ -n "$1" ]; then
			postprocess_only="$1"
			shift
		fi
		;;
		--run-dir)
		shift
		if [ -n "$1" ]; then
			benchmark_run_dir="$1"
			shift
		fi
		;;
		--max-stddev)
		shift
		if [ -n "$1" ]; then
			maxstddevpct="$1"
			shift
		fi
		;;
		--max-failures)
		shift
		if [ -n "$1" ]; then
			max_failures="$1"
			shift
		fi
		;;
		--samples)
		shift
		if [ -n "$1" ]; then
			nr_samples="$1"
			shift
		fi
		;;
		-i|--nr-flows)
		shift
		if [ -n "$1" ]; then
			nr_flows="$1"
			shift
		fi
		;;
		-r|--start-rate-mpps)
		shift
		if [ -n "$1" ]; then
			start_rate_mpps="$1"
			shift
		fi
		;;
		--max-drop-pct|--max-drop-pcts)
		shift
		if [ -n "$1" ]; then
			max_drop_pcts="$1"
			shift
		fi
		;;
		-t|--test-types)
		shift
		if [ -n "$1" ]; then
			test_types="$1"
			shift
		fi
		;;
		--tool-group)
		shift
		if [ -n "$1" ]; then
			tool_group="$1"
			shift
		fi
		;;
		-m|--frame-sizes)
		shift
		if [ -n "$1" ]; then
			frame_sizes="$1"
			shift
		fi
		;;
		-r|--search-runtime)
		shift
		if [ -n "$1" ]; then
			search_runtime="$1"
			shift
		fi
		;;
		-r|--validation-runtime)
		shift
		if [ -n "$1" ]; then
			validation_runtime="$1"
			shift
		fi
		;;
		--log-response-times)
		shift
		if [ -n "$1" ]; then
			log_response_times="$1"
			shift
		fi
		;;
		--portlist)
		shift
		if [ -n "$1" ]; then
			portlist="$1"
			shift
		fi
		;;
		-c|--config)
		shift
		if [ -n "$1" ]; then
			config="$1"
			shift
		fi
		;;
		--start-iteration-num)
		shift
		if [ -n "$1" ]; then
			start_iteration_num=$1
			shift
		fi
		;;
		--)
		shift
		break
		;;
		*)
		error_log "[$script_name] bad option, \"$1 $2\""
		break
		;;
	esac
done
if [[ -z "$benchmark_run_dir" ]]; then
	# We don't have an explicit run directory, construct one
	benchmark_run_dir="$pbench_run/${benchmark_name}_${config}_$date"
else
	# We have an explicit run directory provided by --run-dir, so warn
	# the user if they also used --config
	if [[ ! -z "$config" ]]; then
		warn_log "[$script_name] ignoring --config=\"$config\" in favor of --rundir=\"$benchmark_run_dir\""
	fi
fi
mkdir -p $benchmark_run_dir/.running

total_iterations=0
for test_type in `echo $test_types | sed -e s/,/" "/g`; do
	for max_drop_pct in `echo $max_drop_pcts | sed -e s/,/" "/g`; do
		for frame_size in `echo $frame_sizes | sed -e s/,/" "/g`; do
			for nr_flow in `echo $nr_flows | sed -e s/,/" "/g`; do
				((total_iterations++))
			done
		done
	done
done
count=1
mkdir -p $benchmark_run_dir/.running
export benchmark_name config
pbench-collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir beg
# start the server processes
for test_type in `echo $test_types | sed -e s/,/" "/g`; do
	for max_drop_pct in `echo $max_drop_pcts | sed -e s/,/" "/g`; do
		for frame_size in `echo $frame_sizes | sed -e s/,/" "/g`; do
			for nr_flow in `echo $nr_flows | sed -e s/,/" "/g`; do
				if [ $count -ge $start_iteration_num ]; then
					iteration="${count}-${test_type}-${frame_size}B-${nr_flow}flows-${max_drop_pct}pct_drop"
					iteration_dir="$benchmark_run_dir/$iteration"
					result_stddevpct=$maxstddevpct # this test case will get a "do-over" if the stddev is not low enough
					failures=0
					echo "Starting iteration[$iteration] ($count of $total_iterations)"
					log "Starting iteration[$iteration] ($count of $total_iterations)"
					while [[ $(echo "if (${result_stddevpct} >= ${maxstddevpct}) 1 else 0" | bc) -eq 1 ]]; do
						if [[ $failures -gt 0 ]]; then
							echo "Restarting iteration[$iteration] ($count of $total_iterations)"
							log "Restarting iteration[$iteration] ($count of $total_iterations)"
						fi
						mkdir -p $iteration_dir
						# each attempt at a test config requires multiple samples to get stddev
						for sample in `seq 1 $nr_samples`; do
							benchmark_results_dir="$iteration_dir/sample$sample"
							if [ "$postprocess_only" != "y" ]; then
								mkdir -p $benchmark_results_dir
								echo "test sample $sample of $nr_samples"
								log "test sample $sample of $nr_samples "
								benchmark_client_cmd_file="$iteration_dir/$benchmark_name-client.cmd"
								result_file=$benchmark_results_dir/moongen-result.txt
								pushd >/dev/null $moongen_dir
								if [ $test_type == "unidirec" ]; then
									bidirec_opt=false
								else
									bidirec_opt=true
								fi
								# save benchmark command in file for debugging or running manually
								
								echo "VSPERF {" >$cfg_file
								if [ ! -z "$start_rate_mpps" ]; then
        								echo "startRate = $start_rate_mpps," >>$cfg_file
								fi
        							echo "runBidirec = $bidirec_opt," >>$cfg_file
        							echo "searchRunTime = $search_runtime,">>$cfg_file
        							echo "frameSize = $frame_size,">>$cfg_file
        							echo "validationRunTime = $validation_runtime,">> $cfg_file
        							echo "acceptableLossPct = $max_drop_pct," >>$cfg_file
        							echo "ports = {$portlist}" >>$cfg_file
								echo "}">>$cfg_file
								cp $cfg_file $benchmark_results_dir/$cfg_file

								echo "pushd \>/dev/null $moongen_dir" >$benchmark_client_cmd_file
								echo "./build/MoonGen ./examples/opnfv-vsperf.lua" >>$benchmark_client_cmd_file
								echo "popd \>/dev/null $moongen_dir" >>$benchmark_client_cmd_file
								chmod +x $benchmark_client_cmd_file
								pbench-start-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
								$benchmark_client_cmd_file | tee $result_file
								pbench-stop-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
								pbench-postprocess-tools --group=$tool_group --iteration=$iteration --dir=$benchmark_results_dir
								popd >/dev/null
							else
								if [[ ! -d $benchmark_results_dir ]]; then
									error_log "Results directory $benchmark_results_dir does not exist, skipping post-processing"
									continue
								fi
								echo "Not going to run $benchmark_name.  Only postprocesing existing data"
								log "Not going to run $benchmnark_name.  Only postprocesing existing data"
							fi
							$script_path/postprocess/$benchmark_name-postprocess "$benchmark_results_dir" "$iteration" "$tool_label_pattern" "$tool_group"
						done
						$script_path/postprocess/process-iteration-samples "$iteration_dir" "Mframes_sec" "$maxstddevpct" "$failures" "$max_failures" "$tar_nonref_data" "$keep_failed_tool_data"
						fail=$?
						if [ $fail -eq 1 ]; then
							error_log "This test iteration failed"
							((failures++))
						fi
						if [ $fail -eq 0 -o $failures -ge $max_failures ]; then
							error_log "Moving to the next iteration"
							break
						fi
					done # break out of this loop only if the $result_stddevpct is lower than $maxstddevpct
					echo "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
					log "Iteration $iteration complete ($count of $total_iterations), with 1 pass and $failures failures"
				else
					echo "Skipping iteration $iteration ($count of $total_iterations)"
					log "Skipping iteration $iteration ($count of $total_iterations)"
				fi
				last_test_type="$test_type"
				let count=$count+1 # now we can move to the next iteration
			done
		done
	done
done
$script_path/postprocess/generate-benchmark-summary "$benchmark_name" "$orig_cmd" "$benchmark_run_dir"
pbench-collect-sysinfo --group=$tool_group --dir=$benchmark_run_dir end
rmdir $benchmark_run_dir/.running
